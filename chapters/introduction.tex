% !TEX root = ../main.tex

\chapter{Introduction}
\label{ch:introduction}

% \section{General Notation}
% First, I am defining the notation I used throughout this thesis. For the scalars, I used regular, normal-weight variables such as $x$. Vectors are represented by bold lower-case variables like $\mathbf{x}$. Tensors and matrices are denoted by bold upper-case characters, such as $\mathbf{X}$. In summary:
% \begin{align*}
% x&: \text{Scalar} \\
% \mathbf{x}&: \text{Vector} \\
% \mathbf{X}&: \text{Matrix or tensor}
% \end{align*}
% \todo[inline]{Useful? Everything covered?}

\section{Reinforcement Learning Control Problems}
Control problems appear in the context of both reinforcement learning and control theory. Thus, we often see vocabulary specific to control theory in the context of reinforcement learning. A control problem, generally explained, is a dynamic system described by state variables. Controls (actions) determine how the system will behave in the future. The goal of a control problem is to work out a strategy that causes the system to terminate in the desired target state. Reinforcement learning and approaches in control theory offer a framework to solve such control problems (\cite{bucsoniu2018reinforcement}). Even though the two fields embark on the same problem, the research communities remained disjoint primarily, leading to different approaches to the same problem. Reinforcement learning often makes model-free predictions from data, whereas control theory often defines well-specified models (\cite{recht2018tour}). This thesis will focus on reinforcement learning frameworks to solve control problems.

Reinforcement learning is an area of machine learning that involves learning the optimal behavior to maximize a reward signal inside a dynamic environment. Trial-and-error interactions with the environment mark the learning process. The learner does not know which actions lead to which rewards until he tries them (\cite{kaelbling1996reinforcement}). Reinforcement learning differs from supervised learning, where learning is achieved through a training set with labeled examples. Each sample of the training data represents a description of the situation and a label that marks how the system should react. The learning process aims to generalize the optimal actions given a specific situation so that the system acts correctly in situations outside the training set.

In reinforcement learning, there are no labels available beforehand. In addition, we can also not classify reinforcement learning as unsupervised learning, where we typically try to find a hidden structure in a set of unlabeled data. With reinforcement learning, we do not try to find some structure in the data, but solely try to maximize the reward signal. Therefore, reinforcement learning is considered a third main machine learning paradigm alongside supervised and unsupervised learning.

One additional challenge often overlooked in other branches of machine learning is a focus on the trade-off between \textit{exploration} and \textit{exploitation}. For a high reward, the reinforcement learning agent should take an action that it has tried in the past and was effective. It should therefore exploit the knowledge gained previously. However, to discover this beneficial action, it has to explore actions it has not taken before (\cite{sutton2018reinforcement}).

The agent's goal in a reinforcement learning problem is to drive the system into the desired state by taking the correct sequence of actions. For example, the goal might be to solve non-trivial skill games like Ball-in-a-Cup or Kendama (\cite{kober2010imitation}). The agent's behavior at a particular time is defined by the policy $\pi$. A policy is a mapping from each state $s \in \mathcal{S}$ of the environment to each action $a \in \mathcal{A}(s)$ the agent can perform in that specific state (\cite{sutton2018reinforcement}). The underlying goal in reinforcement learning is for the agent to learn or approximate the optimal policy which determines the best action to take in a particular state.

% We can represent many problems in machine learning, including reinforcement learning problems, as a function mapping an input space into an output space. The output space or \textit{search space} denotes the space of all feasible solutions. In reinforcement learning, the output space contains any action that could be taken given a specific situation. The underlying function is the output of the learning process of an algorithm and is often called the \textit{target function}. Optimally, we would derive the formula of the function explicitly. However, even though we hypothesize such a function exists, we usually do not have enough information to derive a closed form directly. Especially in reinforcement learning, the available information is very sparse. Thus, we aim to approximate the target function with \textit{function approximators} by using the available data. Hereafter, I will often talk about models to solve a reinforcement learning task. The models generally represent function approximators to solve a given problem.

To overcome the mentioned difficulties unique to reinforcement learning, we need to experiment with alternative models and strategies than those used for supervised or unsupervised learning.

%\todo[inline]{Include MDP.}


\section{Classic Reinforcement Learning}
In reinforcement learning, we have an agent (also called a controller) and an environment. The agent can take on actions to interact with the environment. On the other hand, the environment always holds a specific state that is impacted by the agent's actions. As mentioned, the goal is to find the optimal policy while only receiving a scalar reward signal that indicates how well the agent performs. The agent's goal is to maximize the cumulative rewards received by the environment. Therefore, the agent needs to learn optimal behavior only through trial-and-error interaction with the environment. This aspect makes the problem structure much more challenging.

Figure~\ref{fig:RL_agent_env} illustrates the loop between the agent and the environment. The agent receives the observation $o_t$ from the environment based on the environment's current state. However, not all the information about the environment may be present in the observation. If there is missing information about the environment's current state, the environment is only partially observable. If the observation contains full knowledge, the environment is fully observable (\cite{dong2020deep}).
\begin{figure}[!ht]
\centering
\includegraphics[width=.6\textwidth]{RL_agent_env}
\caption[Interaction loop between a reinforcement learning agent and the environment]{
  \textbf{Interaction loop between an reinforcement learning agent and the environment.}
  At timestep $t$, the agent receives the observation $o_t$ from the environment. The agent decides to select action $a_t$ and interacts with the environment. The enviornment changes and returns the next observation $o_{t+1}$ and the reward $r_{t+1}$ to the agent.
 }
\label{fig:RL_agent_env}
\end{figure}
The agent then performs some action $a_t$ in the environment. The environment's state changes according to the received action and returns an observation $o_{t+1}$ based on the updated state of the environment and a reward $r_{t+1}$. One interaction between the agent and the environment represents one \emph{timestep}.

The rewards are computed by an underlying reward function specific to the environment. The reward function is a function that maps the current state of the environment, the current action, and the subsequent state into a scalar value. It can be deterministic or can contain a random component making it stochastic. Depending on the problem, the reward can be sparse or dense. Sparse reward means that the reward function returns zero in most of its domain and only returns a meaningful reward in a few rare settings or at the end of the experiment. That introduces a major challenge since we cannot know which actions lead to success or failure. Optimization problems with this setting are generally challenging to solve. Dense reward environments return a meaningful reward at every step, making the optimization process more manageable.

Learning a policy that maximizes cumulative rewards is generally non-trivial. If the learning algorithm is not explorative enough, it might get stuck in a local optimum and never reach the goal. However, if there is too much exploration, we receive less overall reward. Thus, there needs to be a balance between exploration and exploitation. In addition, there is often a time delay for the reward. The consequence of an action might not be immediately visible to the agent. This problem is known as the credit-assignment problem in the literature (\cite{sutton2018reinforcement}).

We can solve reinforcement learning problems with methods based on value functions or policy search. Methods based on policy search directly search for the optimal policy $\pi^*$ and do not need to maintain a value function model (\cite{8103164}). The state-value function $V$ defines the expected return of each possible state. It indicates how good or bad being in a particular state is. The agent chooses the best action based on the value of the states. The value of state $s$ under policy $\pi$ is defined by $V_\pi(s)$ and indicates the expected return from beginning in state $s$ and following $\pi$ afterward. More formally, we can define $V_\pi(s)$ as
\begin{equation*}
  V_\pi(s) = E_\pi [R_t | s_t = s],
\end{equation*}
where $t$ denotes the timestep, $R_t$ is the cummulative discounted rewards following timestep $t$, and $E_\pi[]$ symbolizes the expected value while following policy $\pi$ (\cite{sutton2018reinforcement}).

Value functions induce a partial order on policies. Policy $\pi$ is better than policy $\pi'$, formally $\pi \geq \pi'$, if and only if $V_\pi(s) \geq V_{\pi'}(s)$ applies to all states $s \in S$. The optimal policy $\pi^*$ leads to the optimal state-value function:
\begin{equation*}
  V_{\pi^*}(s) = \underset{\pi}{\text{max}} V_\pi(s)
\end{equation*}
To find optimal policies and value functions, we can use the \textit{Bellman equation} which divides the value function into the immediate reward and the discounted future reward. The value function then results in:
\begin{equation*}
  V_\pi(s) = E_\pi [r_{t+1} + \gamma V(s_{t+1}) | s_t = s]
\end{equation*}
The discount factor $\gamma$ determines the importance given to the immediate reward and the future rewards.


% \begin{equation}
%   \mathcal{V}_\pi(s) = \mathbf{R}(s, \pi(s)) +\gamma \Sigma_\substack{s' \in S} \mathcal{P}_{s,s'}^{\pi (s)} \mathcal{V}_\pi (s')
% \end{equation}

As interest in neural networks and deep learning increased, deep reinforcement learning, which uses a neural network to estimate policies or value functions, attracted more attention.

%\todo[inline]{Talk about sample efficiency? Include Q-Learning, reward shaping? show more the difference between this and dps}

\subsection{Neural Networks as Models}
\label{subsec:NN}
Artificial neural networks are machine learning algorithms inspired by the functionalities of the human brain. They are applied successfully in many areas including in reinforcement learning problems. In 1958, Frank Rosenblatt implemented the perceptron (\cite{rosenblatt1958perceptron}). The perceptron represents a simplified version of a biological neuron. It was an early type of what later should be called artificial neural networks. On this idea, the concept of neural networks was born and further developed throughout the years.

Neural networks in the modern sense consist of connected neurons or nodes that imitate the biological neurons of the brain, sending signals to each other. A neuron receives one or multiple input values and calculates its output value. The information is then passed onto the next neuron. Figure~\ref{fig:neuron} illustrates the structure of one artificial neuron.
\begin{figure}[ht]
\centering
\includegraphics[width=.7\textwidth]{neuron}
\caption[Illustration of an artificial neuron]{
  \textbf{Illustration of an artificial neuron.}
  The image illustrates how an artificial neuron works. The neuron receives two input values $x_1, x_2$ and one bias term $b$. It first calculates the weighted sum of the inputs. The activation function then defines the neuron's output passed to the next neuron.
}
\label{fig:neuron}
\end{figure}
To calculate the output value, the neuron receives two input values $x_1, x_2$ and one bias term $b$. We often use a bias with neural networks, allowing us to make affine transformations to the data. The neuron uses the assigned weights $\mathbf{w}$ to calculate the weighted sum of input values. In  more detail, the calculation is the following:
\[
  \mathbf{w}^T \mathbf{x} = w_1 x_1 + w_2 x_2 + b
\]
For convenience, the following images of neural networks will not show the bias term explicitly.

The activation function $f$ then defines the output value that should be passed into the next neuron. This example uses a step function with a threshold of 0. Many other functions can be used as activation functions. Another commonly used function is the logistic function. Figure~\ref{fig:sigmoid} shows the formula and a plot of the logistic sigmoid function in 2D.
\begin{figure}[ht]
\centering
\begin{tikzpicture}
\begin{axis}[
     xmin = -5, xmax = 5,
     ymin = 0, ymax = 1,
     legend cell align = {left},
     legend pos = north west,
     width = 0.5\textwidth,
     height = 0.4\textwidth,
     xlabel = \(x\),
     ylabel = {\(sig(x)\)}
   ]
   \addplot[
       smooth,
       thick,
       purple
   ] {1 / (1 + exp(-x))};
   \addlegendentry{
   $sig(x) = \frac{1}{1 + exp(-x)}$
   }
   \addplot[
       smooth,
       thick,
       blue,
       dotted
   ] {0.5};
\end{axis}
\end{tikzpicture}
\caption[Logistic function]{
 \textbf{Logistic function.}
 The function maps an arbitrary input space into the range between 0 and 1. The output of the function can be interpreted as a probability. The sigmoid function helps scale data into a meaningful value.
}
\label{fig:sigmoid}
\end{figure}
The sigmoid function is a mathematical function that maps an arbitrary input space into an output space with a small range, for example, 0 and 1. The function has a characteristic S-shaped curve.

In a neural network, the neurons are arranged in connected layers. A network has at least an input and an output layer. We can further expand it with one or more hidden layers. Depending on the model, the connectivity between the layers differs. Figure~\ref{fig:neural_network_sketch} shows a sketch of a network with three hidden layers that are fully connected.
\begin{figure}[ht]
\centering
\includegraphics[width=.7\textwidth]{neural_network_sketch}
\caption[Sketch of a neural network]{
  \textbf{Sketch of a neural network.}
  The image shows a sketch of a neural network with three fully connected hidden layers. The network expects $n$ inputs and has $m$ outputs.
}
\label{fig:neural_network_sketch}
\end{figure}

A neural network with one or more hidden layers is theoretically able to represent any function. Thus, neural networks are generic function approximators. In addition, neural networks are highly expressive and flexible. In the context of reinforcement learning, they can be used to approximate a value function or a policy function. A large part of the success of neural networks can be traced back to the use of \textit{backpropagation}. Backpropagation uses information from the previous epoch (i.e., iteration) to adjust the network weights using the error gradient. Deep reinforcement learning combines reinforcement learning with the methods of deep learning. Deep neural networks have multiple hidden layers, allowing them to represent data with multiple levels of abstractions (\cite{lecun2015deep}). Deep reinforcement learning has been used successfully in many applications, including robotics, video games, and computer vision (\cite{franccois2018introduction}).

Deep reinforcement learning combines deep learning and reinforcement learning. The main advantage is the scalability of deep neural networks in a high dimensional space. (\cite{dong2020deep})

The backpropagation algorithm's efficiency in calculating the gradient of an objective function is a crucial component of deep learning's success. However, there are certain limitations. For the backpropagation algorithm to be efficient, it relies on calculating accurate gradients. In fields like reinforcement learning, where we only have a reward signal that might be time delayed, this is a non-trivial task. In addition, there is a risk of getting stuck in a local optimum (\cite{ha2017visual}). These limits raise the question of which alternative models or techniques could be used that overcome these difficulties.

%However, neural networks also come with a few limitations or disadvantages. The backpropagation algorithm requires the calculation of an accurate gradient. Depending on the problem, this can be a challenging task. For example, calculating an accurate gradient in RL problems is usually non-trivial. Furthermore, there is a lack of understanding. The output of a neural network is often incomprehensible because of its complex structure.

%\todo[inline]{Write more about DRL -> include recent results, advantages, disadvantages.}

\section{OpenAI Gym}
\label{sec:gym}
The research community requires high-quality benchmarks to compare algorithms and models applied to problems in reinforcement learning. There were already a few released benchmarks like the Arcade Learning Environment (\cite{bellemare2013arcade}) or the RLLab benchmark for continuous control (\cite{duan2016benchmarking}). OpenAI Gym\footnote{https://www.gymlibrary.ml/} combined the elements of these existing benchmarks in one toolkit. It provides a collection of tasks (called \textit{environments}) with varying levels of difficulties and challenges. The interface is the same for all environments, making it very convenient to use. The environments represent general reinforcement learning control problems. To interact with an environment, we first have to initialize it. We can do this with the predefined function \verb|make()|. The following Python code shows the initialization of the environment \verb|CartPole|:
\begin{minted}[linenos, frame=single]{python}
import gym
env = gym.make("CartPole-v0")
\end{minted}
In the setting of reinforcement learning, we assume that an agent is interacting with the dynamic environment. In each step, the agent can take some action and receive a reward and a new observation from the environment. The environment predefines both the action and observation space. In practice, an agent returns an action \verb|a0| with which we call the function \verb|step()| of the environment. The next code snippet shows an example of this behavior:
\begin{minted}[linenos, frame=single]{python}
observation, reward, done, info = env.step(a0)
\end{minted}
The observation and reward returned by the environment apply to the current timestep. OpenAI Gym focuses on episodic reinforcement learning, where the agent acts in a series of episodes with a fixed length. The initial state of the environment in each episode is sampled randomly. An episode ends when specific termination criteria are met. For example, when the task is solved or when we max out on timesteps (\cite{1606.01540}).

\section{Direct Policy Search}
%mention it as an alternative to Deep RL, mention it uses networks to approximate policy rather than value function \\ \\
Although using value functions to solve reinforcement learning problems has worked well in many applications, there are limitations. If the state space is not entirely observable, convergence problems can arise (\cite{el2005direct}). Additionally, in traditional value-based methods like Q-learning or temporal difference learning, the value function is computed iteratively with the help of bootstrapping. That often results in a bias in the quality assessment of state-action pairs for continuous state spaces. Another disadvantage of these methods is that the value functions are often discontinuous (\cite{deisenroth2013survey}).

Studies suggest that directly approximating a policy can be easier and delivers better results (\cite{DBLP:journals/corr/abs-1806-01363}; \cite{sutton1999policy}; \cite{anderson2000approximating}). This alternative method to methods based on value functions is called \textit{direct policy search}. With direct policy search, the algorithm directly searches in the search space of policy representations discarding any value function (\cite{wierstra2010study}). Instead of approximating a value function, we approximate the policy directly with an independent function approximator. Thus, we directly approximate the action-state mapping.

%\todo[inline]{Explain more, go more into advantages, find more papers with comparison}

\section{Neuroevolution}
%special case of DPS: NN as policy approximator, train with evo \\ \\
Neuroevolution is a method that we can use for direct policy search. Like deep reinforcement learning, we employ a neural network as our model. In deep reinforcement learning, we typically use gradient-based learning strategies, whereas, in neuroevolution, we employ evolutionary algorithms to optimize the network parameters. That reduces the risk of being unable to estimate accurate gradients or being stuck in a local optimum. Evolutionary algorithms only require a measure of the network's performance, making them directly applicable for reinforcement learning problems. Thus, we can use it more widely in various problem classes. \cite{such2017deep} showed that genetic algorithms, a subgroup of evolutionary algorithms, work well for the parameter search of neural networks. Even in complex reinforcement learning settings, including Atari and humanoid locomotion, they could achieve results at the scale of traditional deep reinforcement learning frameworks.

%\todo[inline]{Explain more. more with direct policy search}

\section{Black-Box Optimization}
In black-box optimization, the problem structure, as well as the model, is unknown to the learning algorithm. Those methods optimize a parametrization without any constraints on the problem, model or solution needed. Unlike the gradient-based learning techniques there is no constraint on the model or error differentiability. The optimization is solely based on a score or cumulative reward, which makes these methods directly applicable to reinforcement learning problems. Since there are no constraints imposed on the architecture or properties of the model, we can use any function approximator we find suitable for a problem and optimize it with black-box optimization techniques.

In black-box optimization techniques, we optimize a parametrization with the help of the fitness function instead of a reward function. Both are a measure to indicate how well the model performs. Until now, we only talked about reward functions. The reward can be given immediately after combining a state-action pair as in the implementation of OpenAI Gym. The fitness, however, represents the cumulative reward of an episode. In real-life applications, we often only have a cumulative reward instead of getting feedback after each action. Thus, the setting of black-box optimization is more realistic than we see otherwise.

\subsection{Random Weight Guessing}
Random Weight Guessing (RWG) is the simplest black-box optimization method. Algorithm~\ref{alg:RWG} shows the procedure with pseudocode.
\begin{algorithm}
\caption{RWG as described in \cite{schmidhuber2001evaluating}}
\begin{algorithmic}[1]
\Repeat
    \State Randomly sample model weights
\Until{model corretly classifies all training sequences}
\State \textbf{return} model weights
\end{algorithmic}
\label{alg:RWG}
\end{algorithm}
RWG repeatedly randomly initializes the weights of a model maintaining the best so far, monotonically improving performance.

\cite{schmidhuber2001evaluating} were able to show that simple RWG can even outperform previously proposed more complex algorithms. Naturally, RWG is not especially performant. A problem might require many free parameters or high weight precision in practical applications. In these cases, RWG becomes infeasible for finding a suitable parametrization within a reasonable time. A more appropriate learning technique is suggested for real-life applications. Nevertheless, the results suggest that RWG can be used as an analysis method. \cite{oller_analyzing_2020} also used an RWG algorithm to analyze the complexity of some basic OpenAI Gym benchmark problems.

\subsection{Evolution Strategies}
%ES-(1+1) (normal, adapt mean) \\ \\
Evolution Strategies (ES) belong to the class of evolutionary algorithms best suited for continuous optimization. They are directly inspired by the idea of evolution in nature and use the concept of mutation and selection. ES are implemented in a loop where one iteration is called a generation. New individuals (candidate solutions) are generated using the current parents in each generation. We continue with the next generation until a stopping criterion is met. ES are multi-agent algorithms that generate multiple parametrizations that independently explore a different path or area in the optimization space. This exploration prevents us from getting stuck in a local optimum.

\textbf{(1 + 1)-ES:} The population model (1 + 1) means that we maintain only one individual. For the next generation, we generate one new offspring as a variation of the parent, drawn from the normal distribution. For this, we adapt the mean of the distribution according to the parent. Then we keep either the parent or the child based on which had a better fitness score. Algorithm~\ref{alg:ES} shows this process in pseudocode.
\begin{algorithm}
\caption{(1 + 1)-ES in $d$ dimensions}
\begin{algorithmic}[1]
\State Parameter: $\sigma > 0$
\State Initialization: $x = x_0 \in \mathbb{R}^d$
\While{not done}
    \State Sample $x' \sim \mathcal{N}(x, \sigma^2 I)$
    \If{$f(x') \leq f(x)$}
      \State $x \leftarrow x'$
    \EndIf
\EndWhile
\State \textbf{return} $x$
\end{algorithmic}
\label{alg:ES}
\end{algorithm}

\subsection{Covariance Matrix Adaptation Evolution Strategy}
The Covariance Matrix Adaptation Evolution Strategy (CMA-ES) is an advanced ($\mu, \lambda$)-ES that adapts not only the mean of the multivariate normal distribution but also updates the covariance matrix. Since we are in the field of black-box optimization techniques, there are no constraints on the problem, model, or solution of the problem. CMA-ES is a method for non-linear, non-convex optimization problems in a continuous domain. It has been successful in many applications with ill-conditioned objective functions and is considered the industry standard. The following technical information has mainly been summerized from \cite{hansen2016cma}.

\paragraph*{Sampling of the population:} For each iteration of the CMA-ES algorithm, a population of new individuals is generated by sampling a multivariate normal distribution. Formally, for generation $g+1$, the population sampling can be written as
\[
  \mathbf{x}_k^{(g+1)} \sim \mathbf{m}^{(g)} + \sigma^{(g)}  \mathcal{N}(\mathbf{0}, \mathbf{C}^{(g)}) \qquad \text{for} \quad k = 1, ... \lambda,
\]
where $\mathbf{x}_k^{(g+1)}$ symbolizes the $k$-th individual from generation $g+1$, $\mathbf{m}^{(g)}$ denots the mean value of the distribution at generation $g$, $\sigma^{(g)}$ is the standarddeviation at generation $g$, $\mathbf{C}^{(g)}$ is the covariance matrix at generation $g$, and $\lambda$ is the size of the population. After the sampling of generation $g+1$, we need to adapt the mean and the covariance matrix for the sampling of the next generation. Thus, we need to calculate $\mathbf{m}^{(g+1)}$ and $\mathbf{C}^{(g+1)}$.

\paragraph*{Moving the mean:} At the end of each iteration, we need to update the mean of the distribution. The new mean $\mathbf{m}^{(g+1)}$ is a weighted average of $\mu$ selected individuals from the generation $g+1$. Thus, we select $\mu$ individuals from $\mathbf{x}_1^{(g+1)}, ..., \mathbf{x}_\lambda^{(g+1)}$. The formula to update the mean for the next generation is the following:
\[
  \mathbf{m}^{(g+1)} = \mathbf{m}^{(g)} + c_m \Sigma^\mu_{i=1} w_i (\mathbf{x}_{i:\lambda}^{(g+1)} - \mathbf{m}^{(g)})
\]
\[
  \Sigma^\mu_{i=1} w_i = 1, \quad w_1 \geq w_2 \geq ... \geq w_\mu > 0
\]
$c_m \leq 1$ is the learning rate of the algorithm and is usually chosen as $c_m = 1$. $\mu$ is the population size with $\lambda$ being the population size of the parent population with $\mu \leq \lambda$. The weights $w_{i=1,...,\mu} \in \mathbb{R}_{>0}$ are positive coefficients used for the recombination process. The variable $\mathbf{x}_{i:\lambda}^{(g+1)}$ symbolizes the $i$-th best indivdual from the $(g+1)$-th population in relation to its corresponding fitness score.

\paragraph{Adaption of the covariance matrix:} In addition to updating the mean of the search distribution, we also need to adapt the covariance matrix. Calculating the adapted covariance matrix is significantly more complex than updating the mean and takes the most computational effort. More details can be found in \cite{hansen2016cma}.

% for later flat fitness = In the case of equal function values for several individuals in the population

\section{Research Questions}
\label{sec:research_questions}
%Start from the experiments. They are your answer. Which question do they answer? These are your research questions. \\
%In the experiments section, mention explicitly which question each answers \\ \\
The main goal of this thesis is to compare alternative models to neural networks. To analyze and compare the different models and their architectures, I formulated the following research questions:
\begin{enumerate}
  \item How do neural networks compare with other function approximators?
  \item What advantages and disadvantages can we see with other function approximators?
  \item How does the bias influence the performance of the model?
  % \begin{enumerate}
  %   \item Does increasing the number of weights worsen the performance of the model?
  %   \item Does a neural network with more than two hidden layers yield worse scores?
  %   \item Does the neural network's performance suffer from an increase in the number of neurons?
  %   \item Does the difficulty of the environment affect the observed effect of the bias?
  % \end{enumerate}
\end{enumerate}
