% !TEX root = ../main.tex

@article{Reference1,
	Abstract = {We have developed an enhanced Littrow configuration extended cavity diode laser (ECDL) that can be tuned without changing the direction of the output beam. The output of a conventional Littrow ECDL is reflected from a plane mirror fixed parallel to the tuning diffraction grating. Using a free-space Michelson wavemeter to measure the laser wavelength, we can tune the laser over a range greater than 10 nm without any alteration of alignment.},
	Author = {C. J. Hawthorn and K. P. Weber and R. E. Scholten},
	Journal = {Review of Scientific Instruments},
	Month = {12},
	Number = {12},
	Numpages = {3},
	Pages = {4477--4479},
	Title = {Littrow Configuration Tunable External Cavity Diode Laser with Fixed Direction Output Beam},
	Volume = {72},
	Url = {http://link.aip.org/link/?RSI/72/4477/1},
	Year = {2001}}

@article{Reference3,
	Abstract = {Operating a laser diode in an extended cavity which provides frequency-selective feedback is a very effective method of reducing the laser's linewidth and improving its tunability. We have developed an extremely simple laser of this type, built from inexpensive commercial components with only a few minor modifications. A 780~nm laser built to this design has an output power of 80~mW, a linewidth of 350~kHz, and it has been continuously locked to a Doppler-free rubidium transition for several days.},
	Author = {A. S. Arnold and J. S. Wilson and M. G. Boshier and J. Smith},
	Journal = {Review of Scientific Instruments},
	Month = {3},
	Number = {3},
	Numpages = {4},
	Pages = {1236--1239},
	Title = {A Simple Extended-Cavity Diode Laser},
	Volume = {69},
	Url = {http://link.aip.org/link/?RSI/69/1236/1},
	Year = {1998}}

@article{Reference2,
	Abstract = {We present a review of the use of diode lasers in atomic physics with an extensive list of references. We discuss the relevant characteristics of diode lasers and explain how to purchase and use them. We also review the various techniques that have been used to control and narrow the spectral outputs of diode lasers. Finally we present a number of examples illustrating the use of diode lasers in atomic physics experiments. Review of Scientific Instruments is copyrighted by The American Institute of Physics.},
	Author = {Carl E. Wieman and Leo Hollberg},
	Journal = {Review of Scientific Instruments},
	Keywords = {Diode Laser},
	Month = {1},
	Number = {1},
	Numpages = {20},
	Pages = {1--20},
	Title = {Using Diode Lasers for Atomic Physics},
	Volume = {62},
	Url = {http://link.aip.org/link/?RSI/62/1/1},
	Year = {1991}}

	@article{oller_analyzing_2020,
		title = {Analyzing Reinforcement Learning Benchmarks with Random Weight Guessing},
		url = {http://arxiv.org/abs/2004.07707},
		abstract = {We propose a novel method for analyzing and visualizing the complexity of standard reinforcement learning ({RL}) benchmarks based on score distributions. A large number of policy networks are generated by randomly guessing their parameters, and then evaluated on the benchmark task; the study of their aggregated results provide insights into the benchmark complexity. Our method guarantees objectivity of evaluation by sidestepping learning altogether: the policy network parameters are generated using Random Weight Guessing ({RWG}), making our method agnostic to (i) the classic {RL} setup, (ii) any learning algorithm, and (iii) hyperparameter tuning. We show that this approach isolates the environment complexity, highlights specific types of challenges, and provides a proper foundation for the statistical analysis of the task's difficulty. We test our approach on a variety of classic control benchmarks from the {OpenAI} Gym, where we show that small untrained networks can provide a robust baseline for a variety of tasks. The networks generated often show good performance even without gradual learning, incidentally highlighting the triviality of a few popular benchmarks.},
		journaltitle = {{arXiv}:2004.07707 [cs, stat]},
		author = {Oller, Declan and Glasmachers, Tobias and Cuccu, Giuseppe},
		urldate = {2021-12-07},
		date = {2020-04-16},
		eprinttype = {arxiv},
		eprint = {2004.07707},
		keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
		file = {arXiv Fulltext PDF:/home/corina/Zotero/storage/WN7HBCQY/Oller et al. - 2020 - Analyzing Reinforcement Learning Benchmarks with R.pdf:application/pdf;arXiv.org Snapshot:/home/corina/Zotero/storage/X95525AX/2004.html:text/html},
	}

	@article{ha2017evolving,
	  title   = {Evolving Stable Strategies},
	  author  = {Ha, David},
	  journal = {blog.otoro.net},
	  year    = {2017},
	  url     = {https://blog.otoro.net/2017/11/12/evolving-stable-strategies/}
	}

	@article{ha2017visual,
	  title   = {A Visual Guide to Evolution Strategies},
	  author  = {Ha, David},
	  journal = {blog.otoro.net},
	  year    = {2017},
	  url     = {https://blog.otoro.net/2017/10/29/visual-evolution-strategies/}
	}

	@article{brockman2016openai,
	  title={Openai gym},
	  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
	  journal={arXiv preprint arXiv:1606.01540},
	  year={2016}
	}

	@book{fischer2014,
  title={Lineare Algebra},
  author={Fischer, Gerd},
  year={2014},
  publisher={Springer}
	}

	@article{schmidhuber2001evaluating,
  title={Evaluating benchmark problems by random guessing},
  author={Schmidhuber, J{\"u}rgen and Hochreiter, Sepp and Bengio, Yoshua},
  journal={A Field Guide to Dynamical Recurrent Networks},
  pages={231--235},
  year={2001},
  publisher={Wiley-IEEE Press, Hoboken, NJ, USA}
}

@ARTICLE{6313077,
author={Barto, Andrew G. and Sutton, Richard S. and Anderson, Charles W.},
journal={IEEE Transactions on Systems, Man, and Cybernetics},
title={Neuronlike adaptive elements that can solve difficult learning control problems},
year={1983},
volume={SMC-13},
number={5},
pages={834-846},
doi={10.1109/TSMC.1983.6313077}
}

@inproceedings{NIPS1995_8f1d4362,
 author = {Sutton, Richard S},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky and M.C. Mozer and M. Hasselmo},
 pages = {},
 publisher = {MIT Press},
 title = {Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding},
 url = {https://proceedings.neurips.cc/paper/1995/file/8f1d43620bc6bb580df6e80b0dc05c48-Paper.pdf},
 volume = {8},
 year = {1995}
}

@article{montague1999reinforcement,
  title={Reinforcement learning: an introduction, by Sutton, RS and Barto, AG},
  author={Montague, P Read},
  journal={Trends in cognitive sciences},
  volume={3},
  number={9},
  pages={360},
  year={1999},
  publisher={Elsevier}
}

@article{moore1990efficient,
  title={Efficient memory-based learning for robot control},
  author={Moore, Andrew William},
  year={1990},
  publisher={Citeseer}
}

@incollection{kober2010imitation,
  title={Imitation and reinforcement learning for motor primitives with perceptual coupling},
  author={Kober, Jens and Mohler, Betty and Peters, Jan},
  booktitle={From motor learning to interaction learning in robots},
  pages={209--225},
  year={2010},
  publisher={Springer}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@ARTICLE{8103164,
  author={Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  journal={IEEE Signal Processing Magazine},
  title={Deep Reinforcement Learning: A Brief Survey},
  year={2017},
  volume={34},
  number={6},
  pages={26-38},
  doi={10.1109/MSP.2017.2743240}}

	@article{sutton1999policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
  journal={Advances in neural information processing systems},
  volume={12},
  year={1999}
}

@article{anderson2000approximating,
  title={Approximating a policy can be easier than approximating a value function},
  author={Anderson, Charles W},
  journal={Computer Science Technical Report},
  year={2000}
}

@article{franccois2018introduction,
  title={An introduction to deep reinforcement learning},
  author={Fran{\c{c}}ois-Lavet, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G and Pineau, Joelle and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={11},
  number={3-4},
  pages={219--354},
  year={2018},
  publisher={Now Publishers, Inc.}
}

@article{such2017deep,
  title={Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning},
  author={Such, Felipe Petroski and Madhavan, Vashisht and Conti, Edoardo and Lehman, Joel and Stanley, Kenneth O and Clune, Jeff},
  journal={arXiv preprint arXiv:1712.06567},
  year={2017}
}

@article{hansen2016cma,
  title={The CMA evolution strategy: A tutorial},
  author={Hansen, Nikolaus},
  journal={arXiv preprint arXiv:1604.00772},
  year={2016}
}
